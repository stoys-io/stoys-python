{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Stoys (beta)\n",
    "\n",
    "This is beta version. More notebooks and documentation coming next week...\n",
    "\n",
    "Try it in an environemnt like:\n",
    "\n",
    "```bash\n",
    "python3 -m venv env\n",
    "source env/bin/activate\n",
    "python3 -m pip install --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple stoys\n",
    "python3 -m pip install jupyterlab\n",
    "jupyter lab\n",
    "```\n",
    "\n",
    "In a nutshell: Copy the first cell and then look at a few examples of the `.stoys` extension on DataFrame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Stoys\n",
    "\n",
    "Always create first cell as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stoys\n",
    "\n",
    "stoys.init()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init Spark\n",
    "\n",
    "Create a spark session as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data_cache\n",
    "![ -d data_cache/covid-19-data ] || git -C data_cache clone --filter=tree:0 --no-checkout https://github.com/owid/covid-19-data.git covid-19-data\n",
    "!git -C data_cache/covid-19-data checkout 0447906e7fe406e6baf4f1b8f5db58f58b1d1bca\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "You can use SparkIO like in this example. But you don''t have to. It is optional.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stoys.spark.spark_io import SparkIO, SparkIOConfig\n",
    "\n",
    "sparkIOConfig = SparkIOConfig.notebook.copy(\n",
    "    input_paths=[\n",
    "        'data_cache/covid-19-data/public/data/excess_mortality/excess_mortality.csv?sos-format=csv&header=true&inferSchema=true',\n",
    "        'data_cache/covid-19-data/public/data/vaccinations/vaccinations.csv?sos-format=csv&header=true&inferSchema=true',\n",
    "        'data_cache/covid-19-data/public/data/jhu/full_data.csv?sos-table_name=jhu_csv&sos-format=csv&header=true&inferSchema=true',\n",
    "        'data_cache/covid-19-data/public/data/jhu/locations.csv?sos-format=csv&header=true&inferSchema=true',\n",
    "    ],\n",
    "    output_path = 'tmp/stoys_covid',\n",
    ")\n",
    "\n",
    "sparkIO = SparkIO(spark, sparkIOConfig)\n",
    "spark.catalog.listTables()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table('excess_mortality_csv').limit(4).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table('vaccinations_csv').limit(4).toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Magics - %%ssql\n",
    "\n",
    "Or use cell magic `%%ssql` - Spark (or Stoys :)) SQL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ssql?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### display_limit\n",
    "\n",
    "Beware: The rows number you see from `%%ssql` may be incorrect. It is at most 1000 by default.\n",
    "\n",
    "Let's see what happens if we disable it...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ssql --display_limit=0\n",
    "\n",
    "SELECT * FROM excess_mortality_csv AS em JOIN vaccinations_csv AS v ON em.location = v.location\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That either took long or killed you spark driver. The default limits tries to limit these issues!\n",
    "\n",
    "Why was the join that big anyway?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DqJoin\n",
    "\n",
    "One can look at it with DqJoin.\n",
    "\n",
    "BTW: There is incomplete code to discover joins and show suspicious joins automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_sdf = spark.table('excess_mortality_csv')\n",
    "v_sdf = spark.table('vaccinations_csv')\n",
    "\n",
    "em_sdf.stoys.dq_equi_join(v_sdf, ['location'], ['location'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TODO__: Speaking of DqJoin - It will also (soon) display here sample of not just keys but the whole table.\n",
    "It is important because from the uniqueness of primary key rule one would quickly see the actual problem here.\n",
    "`location` is not unique. Combination of `location` and `date` is.\n",
    "\n",
    "Basically this ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_sdf.stoys.dq(rules=[stoys.spark.dq.DqRules.uniqueRule('location')])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets' try the join it again ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ssql mortality_with_vaccination\n",
    "\n",
    "SELECT * FROM excess_mortality_csv AS em FULL JOIN vaccinations_csv AS v ON em.location = v.location AND em.date = v.date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better now. Let's actually compute something ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ssql covid_data\n",
    "\n",
    "SELECT\n",
    "    v.location,\n",
    "    v.date,\n",
    "    j.new_cases AS cases,\n",
    "    j.new_deaths AS deaths,\n",
    "    em.deaths_2020_all_ages AS excess_deaths,\n",
    "    em.average_deaths_2015_2019_all_ages AS average_deaths,\n",
    "    v.daily_vaccinations AS vaccinations\n",
    "FROM excess_mortality_csv AS em\n",
    "FULL JOIN vaccinations_csv AS v ON em.location = v.location AND em.date = v.date\n",
    "FULL JOIN jhu_csv AS j ON v.location = j.location AND v.date = j.date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API\n",
    "\n",
    "There are a couple of ways to use stoys - magics, python api (matching scala) and extension methods.\n",
    "\n",
    "### Stoys magics\n",
    "\n",
    " 1. `%%ssql` - runs sql query, register result and display a few lines from it\n",
    " 2. `%%ssql_dp` - append '_dp' to the existing '%%ssql' cell and re-run it to get data profile\n",
    " 3. `%%ssql_dq` - see dq result from existing query (It has to have correct form! SELECT *, boolean_expr AS rule_name, ...)\n",
    " 4. `%%ssql_aggsum` - aggregate summaries rendering differently tables used for eyeballing aggregate numbers\n",
    "\n",
    "### Python API (matching scala)\n",
    "\n",
    "It is basically the same just without 'io.' package name prefix.\n",
    "\n",
    "Python api can be used as follows:\n",
    "\n",
    "```python\n",
    "from stoys.spark.dq import *\n",
    "\n",
    "rules = [DqRules.uniqueRule(\"location\")]\n",
    "Dq.fromDataFrame(em_sdf).rules(rules).dqResult()\n",
    "```\n",
    "\n",
    "It has the same pacakges, classes, methods, order of arguments. All the data structures coming in and out are the same.\n",
    "Their copy methods also behaves the same as scala.\n",
    "\n",
    "Note: You can even use `from stoys.utils.scala_compat import *` and use things like `Seq(...)` or `Map.empty`.\n",
    "Such code may not be idiomatic python but it allows to copy scala code to python kernel and back. That comes handy at times.\n",
    "\n",
    "### Extension methods\n",
    "\n",
    "Pandas `DataFrame` and Spark `DataFrame` both have registered extension method `.stoys`. Extensions have slightly different api.\n",
    "They are more concise since the DataFrame and SparkSession are already in the extended object.\n",
    "The extensions also use optional arguments instead of builder pattern. We will see which api style will prove more popular.\n",
    "\n",
    "```python\n",
    "em_sdf.stoys.dq(rules=[DqRules.uniqueRule(\"location\")])\n",
    "```\n",
    "\n",
    "Note: the extension will be added also to DataFrames from koalas and spark.pandas packages.\n",
    "Note: The extensions have also the same code in scala.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dq - Quality\n",
    "\n",
    "Use `DqRules` or write your own `DqRule` and use it with python api or "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stoys.spark.dq import *\n",
    "\n",
    "rules = [\n",
    "    # Prefer using common rules\n",
    "    DqRules.notNullRule('location'),\n",
    "    # Or write a raw rule. If your rule is common consider creating your own collection of common rules.\n",
    "    DqRule('cases__non_negative', '(cases IS NULL) OR (cases >= 0)', None, []),\n",
    "    # They can also be read from configuration.\n",
    "    DqRule.from_json('{\"name\": \"deaths__non_negative\", \"expression\": \"(deaths IS NULL) OR (deaths >= 0)\", \"referenced_column_names\": []}'),\n",
    "]\n",
    "fields = [\n",
    "    DqRules.field('location', data_type_json='\"string\"', regexp='[A-Z][a-z]+'),\n",
    "]\n",
    "\n",
    "covid_data.stoys.dq(fields=fields, rules=rules)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dp - Profile\n",
    "\n",
    "One can just use the `%%ssql_dp` magic. Or quickly append '_dp' to the existing '%%ssql' cell and re-run it.\n",
    "\n",
    "Or use any any other api form. For eaxmple extensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table('excess_mortality_csv').stoys.dp()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AggSum - Aggregate Summaries\n",
    "\n",
    "People are great at seeing patterns in the data. A lot of data issues are discovered by spot checking some aggregate values.\n",
    "For that we have AggSum which can render those tables (hopefully) better and diff them with thresholds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ssql_aggsum\n",
    "\n",
    "SELECT\n",
    "    location,\n",
    "    SUM(cases) AS cases,\n",
    "    SUM(deaths) AS deaths,\n",
    "    SUM(excess_deaths) AS excess_deaths,\n",
    "    SUM(average_deaths) AS average_deaths,\n",
    "    SUM(vaccinations) AS vaccinations\n",
    "FROM covid_data\n",
    "GROUP BY location\n",
    "ORDER BY location\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffing\n",
    "\n",
    "As good as we are at seeing patterns we can do much better with some help. Highlighting difference is really useful.\n",
    "Stoys can diff things like data profiles and aggregate summaries. They help spot regressions between two runs very quickly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core import display\n",
    "from stoys.spark.dp import Dp\n",
    "from stoys.ui.dp_ui import dp_result_to_html\n",
    "\n",
    "em_by_continent_query_template = \"\"\"\n",
    "SELECT\n",
    "  *\n",
    "FROM excess_mortality_csv AS em\n",
    "WHERE EXISTS (SELECT true FROM locations_csv AS l WHERE l.location = em.location AND l.continent = \"{continent}\")\n",
    "\"\"\"\n",
    "\n",
    "asia_em_sdf = spark.sql(em_by_continent_query_template.format(continent='Asia'))\n",
    "europe_em_sdf = spark.sql(em_by_continent_query_template.format(continent='Europe'))\n",
    "display.HTML(dp_result_to_html(asia_em_sdf.stoys.dp(), europe_em_sdf.stoys.dp()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TODO] Add better example of profile diff.\n",
    "\n",
    "[TODO] Fix the profiler performance and improve the histogram.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also diff the aggregate summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core import display\n",
    "from stoys.spark.aggsum import AggSum\n",
    "from stoys.ui.aggsum_ui import aggsum_result_to_html\n",
    "\n",
    "cd_aggsum_in_period_query_template = \"\"\"\n",
    "SELECT\n",
    "    location,\n",
    "    SUM(cases) AS cases,\n",
    "    SUM(deaths) AS deaths,\n",
    "    SUM(vaccinations) AS vaccinations\n",
    "FROM covid_data\n",
    "WHERE date BETWEEN '{start_date}' AND '{end_date}'\n",
    "GROUP BY location\n",
    "ORDER BY location\n",
    "\"\"\"\n",
    "\n",
    "cd_aggsum_q1_sdf = AggSum.fromSql(spark, cd_aggsum_in_period_query_template.format(start_date='2021-01-01', end_date='2021-03-31')).aggSumResult()\n",
    "cd_aggsum_q2_sdf = AggSum.fromSql(spark, cd_aggsum_in_period_query_template.format(start_date='2021-04-01', end_date='2021-06-31')).aggSumResult()\n",
    "display.HTML(aggsum_result_to_html(cd_aggsum_q2_sdf, cd_aggsum_q1_sdf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Stoys Tools\n",
    "\n",
    "There are many more things in stoys library which will be documented soon(-ish):\n",
    " 1. `Respahe` is perhaps the most useeful function in all of stoys. Check out the `ReshapeConfig`!\n",
    " 2. Spark export capable to export millions of files templated with styles and plots in one sql query.\n",
    " 3. The main components presented here can do a lot more things like type inference, dq result filtering, a many ui features.\n",
    " 4. The graph component is coming. That one will integrate all the above and bring a lot more on top of that.\n",
    " 5. There is a jdbc database loader, dag runner with layered configs, and number of scala utilities.\n",
    " "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b2696433bca247508eb30dbc4034c0d54b3a7e41510c26021012194fe46073d9"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
